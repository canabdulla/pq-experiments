#parameters
M = $M
subcentroids = $subcentroids
dataset = $dataset
pq = $pq
sep = $sep

#read the data
dir="./data/ml/"
print(toString(dir + dataset))
X = read(dir + dataset + "_X.csv")
y = read(dir + dataset + "_y.csv")


if(pq) {
  #pad the data if columns are not divisible by M
  if(ncol(X) %% M != 0) {
      zeros = matrix(0, rows=nrow(X), cols= ((ncol(X) %/% M) +1) * M - ncol(X) )
      X = cbind(X, zeros)
  }
  #perform clustering
  print(toString("Starting Clustering with parameters: M=" + M + " subcentroids=" + subcentroids + " sep=" + sep))
  [codebook, X] = quantizeByCluster(X, M, subcentroids, 1, 100, 1e-4, nrow(X) %/% subcentroids, sep, 2)
  print(toString("Clustering completed."))
}

# one hot encoding
m = nrow(X)
n = ncol(X)
fdom = colMaxs(X);
foffb = t(cumsum(t(fdom))) - fdom;
foffe = t(cumsum(t(fdom)))
rix = matrix(seq(1,m)%*%matrix(1,1,n), m*n, 1)
cix = matrix(X + foffb, m*n, 1);
X2 = table(rix, cix); #one-hot encoded

[Xtrain,Xtest,ytrain,ytest] = split(X=X2,Y=y, f=0.7)

# model = ffTrain(X=Xtrain, Y=ytrain, out_activation="sigmoid", loss_fcn="cel")
# yhat = ffPredict(model=model, X=Xtest)
# learn model
print(toString("Starting regression."))

if (dataset != "KDD98") {

    #regression
    B = multiLogReg(X=Xtrain, Y=ytrain, icpt=1, reg=0.00001, verbose=FALSE);
    [Matrix,yhat,acc] = multiLogRegPredict(X=Xtest, B=B, Y=ytest, verbose=FALSE);
    e = (ytest!=yhat);
    print(toString("Regression completed."))

    #calculate statistics
    [conf_a, conf_r] = confusionMatrix(yhat,ytest)
    #compute precision, recall and f-score for every class
    precision = matrix(0, nrow(conf_a), 1)
    recall = matrix(0, nrow(conf_a), 1)
    f_score = matrix(0, nrow(conf_a), 1)

    for(i in 1:ncol(conf_a)) {
        precision[i] = conf_a[i, i] / as.scalar(rowSums(conf_a[i,]))
        recall[i] = conf_a[i, i] / as.scalar(colSums(conf_a[,i]))
        f_score[i] = 2 * (recall[i] * precision[i]) / (recall[i] + precision[i])
    }
    stats = matrix(0, nrow(conf_a)+1, 4)
#     stats[2:nrow(conf_a)+1, 2] = precision
#     stats[2:nrow(conf_a)+1, 3] = recall
#     stats[2:nrow(conf_a)+1, 4] = f_score

    #acc
    stats[1,1] = 1 - (sum(e) / nrow(ytest))
    #avg_precision
    avg_precision = as.scalar(colSums(precision)) / nrow(precision)
    stats[1,2] = avg_precision
    #avg_recall
    avg_recall = as.scalar(colSums(recall)) / nrow(recall)
    stats[1,3] = avg_recall
    #macro_f1
    stats[1,4] = 2 * (avg_precision * avg_recall) / (avg_precision + avg_recall)
    res = stats[1,1:4]
}
else {
    #run linear regression
    B = lm(X=X2, y=y, icpt=1, reg=0.0001, verbose=TRUE);
    print(toString("Regression completed."))
    yhat = X2 %*% B[1:ncol(X2),] + as.scalar(B[ncol(X)+1,]);
    e = (y-yhat)^2;
    #yhat = lmPredict(X=X2, B=B, ytest=y);
    #compute Mean Squared Error
    res = colSums(e)
}

if(pq) {
    out_file = "output/ml/PQ " + dataset + " " + M + " " + subcentroids + " " + sep
}
else {
    out_file = "output/ml/Baseline " + dataset + " " + M + " " + subcentroids + " " + sep
}
write(res, out_file, format="csv")