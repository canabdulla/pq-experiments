compute_distance_matrix = function(Matrix[double] x)
  return (Matrix[Double] D) {
    sq = matrix(0, cols=1, rows=nrow(x))
    ones = matrix(1, cols=1, rows=nrow(x))
    #compute hadamard product
    sq = x^2
    #compute distance matrix
    D = -2 * (x %*% t(x)) + sq %*% t(ones) + ones %*% t(sq)
}

M = $M
subcentroids = $subcentroids
out_file = "output/" + $out_file
space_decomp = $space_decomp
# space_decomp = TRUE


dataset = "siftsmall_base" #$dataset

v = read("data/ann/" + dataset + ".csv", format="csv")


rows=nrow(v)
cols=ncol(v)

#pad the data if number of columns is not divisible by M
if(ncol(v) %% M != 0) {
  zeros = matrix(0, rows=nrow(v), cols= ((ncol(v) %/% M) +1) * M - ncol(v) )
  v = cbind(v, zeros)
  rows=nrow(v)
  cols=ncol(v)
}
subvector_size = ncol(v) / M

#perform space decomposition
if(space_decomp) {
  #mittelwertbefreiung
  col_means = colMeans(v)
  ones = matrix(1, rows=nrow(v), cols=1)
  v2 = v - (ones %*% col_means)
  [U, S, V] = svd(v2)
  singular_v = diag(S)
  eigen_v = singular_v^2 / (nrow(v)-1)
  R = matrix(0, rows=nrow(V), cols=ncol(V))
  buckets = matrix(1, rows=M, cols=1)
  bucket_idx = matrix(1, rows=M, cols=1)
  for(i in 1:M) {
    bucket_idx[i] = bucket_idx[i] + (i-1) * subvector_size
  }
  full_buckets = matrix(0, rows=M, cols=1)
  for(i in 1:nrow(V)) {
    smallest_b = order(target=buckets, by=1, decreasing=FALSE, index.return=TRUE)
    #find empty bucket
    not_found=TRUE
    j = 1
    while(not_found) {
      if(as.scalar(full_buckets[as.scalar(smallest_b[j])]) == 0) {
        b = as.scalar(smallest_b[j])
        not_found = FALSE
      }
    j = j + 1
    }
    buckets[b] = buckets[b] * eigen_v[i]
    R[,as.scalar(bucket_idx[b])] = V[,i]
    bucket_idx[b] = bucket_idx[b] + 1
    if(as.scalar(bucket_idx[b]) > b * subvector_size) {
      full_buckets[b] = 1
    }
  }
  v = v %*% t(R)
}


#perform clustering
print(toString("Starting Clustering with parameters: M=" + M + " subcentroids=" + subcentroids + " sep=TRUE"))
[codebook, codes] = quantizeByCluster(v, M, subcentroids, 1, 1000, 1e-8, (rows/subcentroids), TRUE, 2)
print(toString("Clustering completed."))

#compute distance matrices and save them in D
l = subcentroids
D = matrix(0, cols=l, rows=l*M)
for(j in 1:M) {
    subc = codebook[(j-1)*l+1:j*l,]
    D[(j-1)*l+1:j*l,] = dist(subc) #compute_distance_matrix(subc)
}


query_vectors = read("data/ann/siftsmall_query.csv", format="csv")

truth_vectors = read("data/ann/siftsmall_truth.csv", format="csv")
truth_vectors = truth_vectors[1:nrow(truth_vectors), 1]
retrieved = 0

for(c in 1:100) {
    query_v = query_vectors[c,]
    if(space_decomp) {
      query_v = query_v %*% t(R)
    }
    # query_c = t(codes[query_v,])
    query_c = matrix(0, cols=1, rows=M)

    #compute codes of query vector
    subv_s = ncol(v) / M
    ones = matrix(1, cols=1, rows=l)
    for(i in 1:M) {
        subv = query_v[1, (i-1)*subv_s +1: i * subv_s]
        st = rbind(codebook[(j-1)*l+1:j*l,],subv)
        d = dist(st)
        d = t(d[nrow(d), 1:ncol(d)-1])
        x = order(target=d, by=1, decreasing=FALSE, index.return=TRUE)
        query_c[i] = x[1]
    }

    # compute distances from query_v to all other vectors
    d = matrix(0, cols=1, rows=nrow(codes))
    for(i in 1:nrow(codes), check=0) {
        comp_c = t(codes[i,])
        distance = 0
        for(j in 1:M) {
            qc = query_c[j]
            distance += as.scalar(D[as.scalar(comp_c[j]), as.scalar(qc)])
        }
        d[i] = distance
    }
    x = order(target=d, by=1, decreasing=FALSE, index.return=TRUE)
    x = x[1:100]
    nearest_neighbor = as.scalar(truth_vectors[c])+1
    retrieved_correctly = contains(target=x, pattern=nearest_neighbor)
    if(retrieved_correctly) {
        retrieved = retrieved +1
    }
}
print(toString(retrieved/100))
recall = retrieved / 100
write(recall, out_file)
